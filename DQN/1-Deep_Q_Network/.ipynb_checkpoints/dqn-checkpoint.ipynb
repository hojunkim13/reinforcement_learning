{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySGWc26W2cu4"
   },
   "source": [
    "# Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4rXEFCQR2cAC"
   },
   "source": [
    "It will be implemented a DQN agent with OpenAI Gym's LunarLander-v2 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n",
    "\n",
    "Rocket trajectory optimization is a classic topic in Optimal Control.\n",
    "\n",
    "According to Pontryagin's maximum principle it's optimal to fire engine full throttle or turn it off. \n",
    "\n",
    "That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "\n",
    "To understand LunarLander\n",
    "- Landing pad is always at coordinates (0,0). \n",
    "- The coordinates are the first two numbers in the state vector. \n",
    "- Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. \n",
    "- If lander moves away from landing pad it loses reward back. \n",
    "- Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. \n",
    "- Each leg with ground contact is +10 points.\n",
    "- Firing the main engine is -0.3 points each frame. \n",
    "- Firing the side engine is -0.03 points each frame.\n",
    "- Solved is 200 points.\n",
    "- Landing outside the landing pad is possible. \n",
    "- Fuel is infinite, so an agent can learn to fly and then land on its first attempt. \n",
    "\n",
    "Four discrete actions available: \n",
    "1. Do nothing.\n",
    "2. Fire left orientation engine.\n",
    "3. Fire main engine.\n",
    "4. Fire right orientation engine.\n",
    "\n",
    "Please see the source code for details.\n",
    "https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py\n",
    "- To see a heuristic landing, run: python gym/envs/box2d/lunar_lander.py\n",
    "- To play yourself, run: python examples/agents/keyboard_agent.py LunarLander-v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://github.com/RMiftakhov/LunarLander-v2-drlnd\n",
    "    \n",
    "https://www.katnoria.com/nb_dqn_lunar/\n",
    "    \n",
    "https://drawar.github.io/blog/2019/05/12/lunar-lander-dqn.html\n",
    "    \n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn Reinforcement Learning (3) - DQN improvement and Deep SARSA\n",
    "\n",
    "https://greentec.github.io/reinforcement-learning-third-en/\n",
    "\n",
    "\n",
    "Introduction to Double Deep Q Learning (DDQN)\n",
    "\n",
    "https://mc.ai/introduction-to-double-deep-q-learning-ddqn/\n",
    "\n",
    "\n",
    "CONTINUOUS  CONTROL  WITH  DEEP  REINFORCEMENTLEARNING\n",
    "\n",
    "https://arxiv.org/pdf/1509.02971.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other references:\n",
    "\n",
    "https://cugtyt.github.io/blog/rl-notes/201807201658.html\n",
    "\n",
    "https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/\n",
    "\n",
    "\n",
    "https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n",
    "\n",
    "\n",
    "https://adgefficiency.com/dqn-tuning/\n",
    "\n",
    "https://stackoverflow.com/questions/57106676/weird-results-when-playing-with-dqn-with-targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the nacessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formmat = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. instantiate the environmrnt and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Nunber of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Nunber of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=8, action_size=4, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatch an untrained agent\n",
    "state = env.reset()\n",
    "for j in range(6000):\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    '''\n",
    "    Deep Q-Learning\n",
    "    \n",
    "    Params:\n",
    "    =======\n",
    "    n_episodes (int): maximum number of training episodes\n",
    "    max_t (int): maximum number of timesteps per episode\n",
    "    eps_start (float): starting epsilon value (epsilon-greedy action selection)\n",
    "    eps_end (float): minimum epsilon value\n",
    "    eps_decay (float): multiplicative factor for decreasing epsilon\n",
    "    '''\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window))0)\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dqn.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
